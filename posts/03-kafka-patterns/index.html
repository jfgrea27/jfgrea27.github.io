<!doctype html><html lang=en-uk dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://jfgrea27.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jfgrea27.github.io/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jfgrea27.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=192x192 href=https://jfgrea27.github.io/favicon/android-chrome-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://jfgrea27.github.io/favicon/apple-touch-icon.png><link rel=alternate type=application/rss+xml href=https://jfgrea27.github.io/posts/03-kafka-patterns/index.xml title="James Rea Blog"><meta name=description content><title>Kafka: a practical exploration | James Rea Blog</title><link rel=canonical href=https://jfgrea27.github.io/posts/03-kafka-patterns/><meta property="og:url" content="https://jfgrea27.github.io/posts/03-kafka-patterns/"><meta property="og:site_name" content="James Rea Blog"><meta property="og:title" content="Kafka: a practical exploration"><meta property="og:description" content="Exploration of Kafka configurations."><meta property="og:locale" content="en_uk"><meta property="og:type" content="website"><link rel=stylesheet href=/assets/combined.min.92769f0c8addb7bcf9db875e83d0cb8f2148f28109b20f98f899ed3f7161b495.css media=all></head><body class=light><div style="position:fixed;top:20px;right:20px;z-index:1000;display:flex;align-items:center;gap:12px;background:var(--background);padding:8px 12px;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,.1)"><a href=https://www.linkedin.com/in/james-rea/ target=_blank rel="noopener noreferrer" title=LinkedIn style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://github.com/jfgrea27 target=_blank rel="noopener noreferrer" title=GitHub style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=mailto:jfgrea27@gmail.com title=Email style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a href=/James_Rea___CV___December_2024___v4.pdf target=_blank title=CV style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
</a><a href=https://www.buymeacoffee.com/jfgrea27 target=_blank><img src=https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png alt="Buy Me A Coffee" style=height:40px!important;width:145px!important></a></div><div class=content><header><div class=header></div></header><main class=main><div class=list-container><div class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/>Posts</a>
<span class=breadcrumbs-separator>> </span><a class=breadcrumbs-current href=/posts/03-kafka-patterns/>Kafka: a practical exploration</a></div><h1>Kafka: a practical exploration</h1><p><a href=https://kafka.apache.org/>Apache Kafka</a> has become the <em>de facto</em> event streaming and processing platform in the industry.</p><p>This set of articles explores Kafka through practical experiments, both as a learning tool as well as a resource for anyone who land on this side. All the code can be found in my <a href=https://github.com/jfgrea27/kafka-patterns>kafka-patterns</a> repository. This article will only show the screen recordings of the experiments, but you can ofcourse look at the code directly for each example by looking at the repository. The code is written in Java, the prefered language for Kafka applicatoins.</p><h1 class=heading id=kafka-in-one-paragraph>Kafka in one paragraph
<a class=anchor href=#kafka-in-one-paragraph>#</a></h1><p>Kafka is an event streaming platform and stream processing platform.</p><p>In short, messages are send by producers to a topic in the Kafka cluster. The leader broker in the topic&rsquo;s replica set will save the message and replicate the data across the replicas. Ordering of messages in topics is not guaranteed, however, topics have partitions, where the ordering of message consumption in given partition in topic is guaranteed. Consumers are organised in consumer groups per topic. Each consumer in consumer group can be assigned one or more partitions to process. Consumers commit offsets to store their current progress in processing the messages.</p><p>Please refer to <a href=#terminology>Terminology</a> for a quick summary of the nomeclature, or the Kafka documentation for more details.</p><h1 class=heading id=kafka-patterns>Kafka patterns
<a class=anchor href=#kafka-patterns>#</a></h1><p>We will explore 5 patterns for now:</p><ul><li><a href=#01-simple-producer--consumer>01 simple producer & consumer</a>;</li><li><a href=#02-partition-producer--consumer>02 topic partition in producer & consumer</a>;</li><li><a href=#03-ordering-of-messages-in-partition>03 ordering of messages in topics/partitions</a></li><li><a href=#04---partition-vs-throughputparallelismscaling>04 partition vs throughput</a></li><li><a href=#05---producer-delivery-semantics>05 producer delivery semantics</a></li><li><a href=#06---consumer-delivery-semantics>06 consumer delivery semantics</a></li></ul><h4 class=heading id=01---simple-producer--consumer>01 - Simple producer & consumer
<a class=anchor href=#01---simple-producer--consumer>#</a></h4><p>In this demo, we explore a very basic producer/consumer.</p><p>The producer will continuously send messages every 5 seconds (configurable with <code>SEND_INTERVAL_SECONDS</code>). The consumer will consume from broker. Press Ctrl+C to stop either application gracefully.
Here we see that the producer publishes messages to the <code>demo.topic</code> and these are picked up by the consumer.</p><p><video src=01-simple-producer-consumer/demo.mov controls title="Simple producer and consumer demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>You can see that messages are published to all partitions (0 and 1).
You can see that messages are consumed from all partitions (0 and 1).</p><h4 class=heading id=02---partition-producer--consumer>02 - Partition producer & consumer
<a class=anchor href=#02---partition-producer--consumer>#</a></h4><p>In this demo, we explore the concept of <a href=#partition>partitions</a> in Kafka. A given topic can be split into partitions. Messages are written to one partition, determined by their partition key.</p><p>We can define a <code>Partitioner</code> class that will generate a partition key for a given message. <code>ConstantPartitioner</code> will always send to <code>partition=0</code>.</p><p><video src=02-partition-producer-consumer/demo.mov controls title="Example of partitioned topics" style="max-width:600px;display:block;margin:0 auto"></video></p><p>As you can see, consumer consuming from partition 0 (top left) will receive all the message, whilst consumer consuming from partition 1 (bottom left) will receive none of the messages. This shows that consumers that subscribed to a specific partition will only read messages on that partition.</p><h4 class=heading id=03---ordering-of-messages-in-partition>03 - Ordering of messages in partition
<a class=anchor href=#03---ordering-of-messages-in-partition>#</a></h4><p>As explained in <a href=#background>background</a> section, messages are not guaranteed to be consumed in the right order in a given topic, but ordering is guaranteed in a partition in a topic is guaranteed.</p><p>We will use the same <code>Partitioner</code> class in <a href=#02-partition-producer--consumer>02 topic partition in producer & consumer</a>, but this time have the producer publish to both partitions. You should see that the ordering of the messages is preserved in each partition, namely, the monotonically incresing, although, a give partition may consume newer messages that were sent after messages in other partitions.</p><h4 class=heading id=04---partition-vs-throughputparallelismscaling>04 - Partition vs {throughput,parallelism,scaling}
<a class=anchor href=#04---partition-vs-throughputparallelismscaling>#</a></h4><p>Tweaking the partition count per topic impacts throughput and parallelism</p><ul><li>Throughput: the smaller the partition, the more messages per partition, meaning broker has less resources to handle writes -> lower throughput.</li><li>Parallelism: Each partition offers another degree of parallelism. More partitions -> more consumers can run in the consumer group, increasing parallelism.</li></ul><p>Too many partitions can cause severe issues, including:</p><ul><li>Resource intensive on brokers: each partition requires resources (CPU, memory, IO, etc.).</li><li>Failover slows: rebalancing load (e.g. consumer failure) take more time since Kafka coordinator needs to reallocate all partitions across replicas.</li></ul><p>From the consumer&rsquo;s perspective, if a topic has several partitions spread across several brokers, the throughput is larger since the read load on the brokers wil be spread across the brokers.
From the producer&rsquo;s perspective, if a topic has several partitions spread across several brokers, the throughput is larger since the write load on the brokers will be spread across the brokers.</p><p>Overall the right level of partitioning is required to maximise scalability and speed of the Kafka application.</p><p>Below, we will run experiments with 1, 10 and 1000 partition(s) size topics, where a producer will publish as many messages as possible to the topic and then measure <em>consumer lag</em>. Consumer lag, the number of messages behind those published to partition, indicates whether consumers are keeping up producers. The consumers in this experiment will only run for 10 seconds from when their consume their first message.</p><p>We would expect that larger partition will reduce consumer lags.</p><p>Here are the <a href=04-partitions-vs-throughput/results.csv>results</a> for each partition count:</p><div style=display:flex;justify-content:center><table><thead><tr><th style=text-align:left>Topic</th><th style=text-align:left>Partitions</th><th style=text-align:left>Total Lag</th><th style=text-align:left>Average Lag</th></tr></thead><tbody><tr><td style=text-align:left>topic-1</td><td style=text-align:left>1</td><td style=text-align:left>4121</td><td style=text-align:left>4121.00</td></tr><tr><td style=text-align:left>topic-10</td><td style=text-align:left>10</td><td style=text-align:left>3515</td><td style=text-align:left>351.50</td></tr><tr><td style=text-align:left>topic-1000</td><td style=text-align:left>1000</td><td style=text-align:left>1478</td><td style=text-align:left>1.48</td></tr></tbody></table></div><p>As you can see, the average lag reduces with larger topics, which supports the theory on throughput and parallelism.</p><p>Creating 10000 topics crashes the container, which also supports the resource burden partitions cause.</p><h4 class=heading id=05---producer-delivery-semantics>05 - Producer delivery semantics
<a class=anchor href=#05---producer-delivery-semantics>#</a></h4><p>As described in <a href=#delivery-semantics>delivery semantics</a>, there are 3 kinds of delivery semantics for the producer:</p><ul><li>At-least-once;</li><li>At-most-once;</li><li>Exactly-once.</li></ul><p>In this section, we will look at delivery semantics from the perspective of the producer.</p><p><em>At-leace-once</em></p><p>The setup up for at-least-once semantic is as follows:</p><pre tabindex=0><code># producer setup
acks=all # waits all in-sync replicas (ISR) acknowledgement
retries&gt;0 # retries if record is not sent
enable.idempotence=false # will not deduplicate messages if received twice
</code></pre><p>To simulate multiple same message sent, we will use <a href=https://github.com/Shopify/toxiproxy>toxiproxy</a> to block acknowledgements:</p><p><video src=05-producer-delivery-semantics/at-least-once-demo.mov controls title="At-least-once delivery demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>Here is a demo where we set up simple producer/consumer, then add latency via the <code>toxiproxy</code>, then remove the latency and see that a consumer will consume the same message twice.</p><p><em>At-most-once</em></p><p>The setup for at-most-once semantic is as follows:</p><pre tabindex=0><code># producer setup
acks=0 # don&#39;t acknowledge, just send next message
retries=0 # don&#39;t retry if failed
</code></pre><p>We will apply the same simulation as for <em>at-least-once</em> delivery semantic above but will see that no message is sent twice:
<video src=05-producer-delivery-semantics/at-most-once-demo.mov controls title="At-most-once delivery demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>Here is a demo where we set up simple producer/consumer, then add droppage via the <code>toxiproxy</code>, then remove the latency and see that a consumer will have a gap between the messages in the topic, meaning some messages were not sent.</p><p><em>Exactly-once</em></p><p>The setup for exactly-once semantic is as follows:</p><pre tabindex=0><code># producer setup
acks=0 # don&#39;t acknowledge, just send next message
retries=0 # don&#39;t retry if failed
enable.idempotence=true # deduplicate messages if they arrive twice
transactional.id=demo-tx-1
</code></pre><p>Here we see that Kafka requires a transaction to apply exactly-once delivery semantics. This transaction allows producers to send messages atomically to one/more topics (a.k.a. all or nothing).</p><p><video src=05-producer-delivery-semantics/exactly-once-demo.mov controls title="Exactly-once delivery demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>As you can see, no message is dropped/delivered twice, regardless of the send/ack dropage simulated via toxiproxy.</p><p>Overall, this shows that different settings will impact delivery semantics. It is worth noting that depending on the application, a certain delivery semantic might be more suited. For example, low latency events like view clicks on a video most will probably not require accurate aggregate value, and so at-most-once delivery semantic is useful. Conversely, data analytics pipelines where crucial duplicate results can be cleaned up async will require at-least-once delivery semantics and finally exactly-once delivery semantics might be required in crucial cases such as financial transactions, where dropped or duplicate messages can be very costly. You must decide which delivery semantic makes most sense given the context.</p><p>At-most-once has the lowest overheads since no ack is required.
At least-once will require some acknowledgments, which increases overheads.
Exactly-once will require the most overhead since a transaction, idempotency, retries and acks are all required. This will have performance impacts, but are worth the trade off in crucial contexts.</p><h4 class=heading id=06---consumer-delivery-semantics>06 - Consumer delivery semantics
<a class=anchor href=#06---consumer-delivery-semantics>#</a></h4><p>Consumers commit their progress as <a href=#offset>offset</a>. This offset dictates whether a given user has consumed the message. Crucially, how the application commits this offset will impact the delivery semantic from the consumer&rsquo;s perspective. There are 3 cases:</p><ul><li>At-most-once: Consumer will read the message, commit message consumed and then process it. If the consumer fails during processing, the consumer won&rsquo;t consume the message again when back up.</li><li>At-least-once: Consumer will read the message, process it and then commit message consumed. If the consumer fails just after processing, the consumer will consume the message again since the offset was not committed.</li><li>Exactly-once: Consumer will messages that have been committed in the transaction <code>read_committed</code>.</li></ul><p>We will explore 3 demos for each case:</p><p><em>At-most-once</em></p><p>Here is the setup for at-most-once:</p><pre tabindex=0><code># consumer config
enable.auto.commit=true
auto.commit.interval.ms=5
</code></pre><p>We will set up a consumer that will read, commit and fail processing in a loop.</p><p><video src=06-consumer-delivery-semantics/at-most-once-demo.mov controls title="Consumer at-most-once delivery demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>As you can see, the consumer fails on every odd message, and it doesn&rsquo;t retry since it has already committed its consumption of these messages prior to processing them.</p><p><em>At-least-once</em></p><p>Here is the setup for at-most-once:</p><pre tabindex=0><code># consumer config
enable.auto.commit=false
</code></pre><p>We will set up a consumer that will read, commit and fail processing in a loop.</p><p><video src=06-consumer-delivery-semantics/at-least-once-demo.mov controls title="Consumer at-least-once delivery demo" style="max-width:600px;display:block;margin:0 auto"></video></p><p>As you can see, the consumer fails on every odd message, and it seeks to known offset and then rereads messages that have already been sent.</p><h1 class=heading id=terminology>Terminology
<a class=anchor href=#terminology>#</a></h1><h2 class=heading id=topic><em>Topic</em>:
<a class=anchor href=#topic>#</a></h2><p>A category/feed where messages are published to. Order of messages in topics are <strong>not guaranteed</strong>, only within partition.</p><h1 class=heading id=broker><em>Broker</em>
<a class=anchor href=#broker>#</a></h1><p>A Kafka server that stores messages and serves them to clients. When a partition is replicated, one of the partition&rsquo;s replica broker is the leader broker. All messages are published to this broker and then replicated to the other brokers in the replication set. If the leader broker fails, new leader broker election is triggered.</p><h2 class=heading id=producer><em>Producer</em>
<a class=anchor href=#producer>#</a></h2><p>A client that sends messages to broker on a topic.</p><h2 class=heading id=consumer><em>Consumer</em>
<a class=anchor href=#consumer>#</a></h2><p>A client that reads messages from broker&rsquo;s topic.</p><h2 class=heading id=partition><em>Partition</em>
<a class=anchor href=#partition>#</a></h2><p>A subdivision of a topic. Order of messages in partition in a topic are guaranteed.</p><h2 class=heading id=consumer-group><em>Consumer group</em>
<a class=anchor href=#consumer-group>#</a></h2><p>A consumer group is made up of consumers that will subscribe to messages on a topic. Consumers are assigned one or more partitions in the topic and process events here. No two consumers will be assigned the same partitions to preserve ordering.</p><h2 class=heading id=offset><em>Offset</em>
<a class=anchor href=#offset>#</a></h2><p>Consumers consume messages and commit an offset. Replaying history is possible.</p><h2 class=heading id=replication><em>Replication</em>
<a class=anchor href=#replication>#</a></h2><p>A replication of a Kafka Broker.</p><h2 class=heading id=delivery-semantics><em>Delivery Semantics</em>
<a class=anchor href=#delivery-semantics>#</a></h2><p>Delivery semantics describes what guarantees a sender/receiver has when sending/receiver a record in Kafka.</p><p>Delivery semantics can be analysed from both the producer and consumer&rsquo;s perspective.</p><p>There are 3 kinds of delivery semantics:</p><ul><li>At-least-once semantic: messages are produced/consumed at least once, meaning duplicates sends/reads are possible.<ul><li>Producer: Retries>0, Acknowledgement of message delivery to all brokers is expected from producer. Loss of ack might lead to duplicate message sent.</li><li>Consumer: If producer sends duplicates, consumer will consume more than once.</li></ul></li><li>At-most-once semantic: messages are produced/consumed at most once, meaning message might be read/send.<ul><li>Producer: Retries=0, Acknowledgement of message delivery to all brokers is not expected from producer. If producer fails before leader ack, then no retry -> loss message.</li></ul></li><li>Exactly-once semantic: messages are produced/consumed exactly once.<ul><li>Producer: Idempotent producer - same messages to partition on topic will be deduplicated.</li><li>Consumer: Complex set up, requires Kafka transactions (<code>isolation.level=read_committed</code>).</li></ul></li></ul></div></main></div><footer><p>Powered by
<a href=https://gohugo.io/>Hugo</a>
and
<a href=https://github.com/tomfran/typo>tomfran/typo</a></p></footer></body><script src=/js/theme-switch.js></script><script defer src=/js/copy-code.js></script></html>