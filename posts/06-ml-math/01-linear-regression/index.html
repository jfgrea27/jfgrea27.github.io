<!doctype html><html lang=en-uk dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://jfgrea27.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jfgrea27.github.io/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jfgrea27.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=192x192 href=https://jfgrea27.github.io/favicon/android-chrome-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://jfgrea27.github.io/favicon/apple-touch-icon.png><meta name=description content><title>01 - Linear regression | James Rea Blog</title><link rel=canonical href=https://jfgrea27.github.io/posts/06-ml-math/01-linear-regression/><meta property="og:url" content="https://jfgrea27.github.io/posts/06-ml-math/01-linear-regression/"><meta property="og:site_name" content="James Rea Blog"><meta property="og:title" content="01 - Linear regression"><meta property="og:description" content="Fitting a line of best fit."><meta property="og:locale" content="en_uk"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-31T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-31T00:00:00+00:00"><meta property="article:tag" content="Math"><meta property="article:tag" content="Ai"><link rel=stylesheet href=/assets/combined.min.92769f0c8addb7bcf9db875e83d0cb8f2148f28109b20f98f899ed3f7161b495.css media=all></head><body class=light><div style="position:fixed;top:20px;right:20px;z-index:1000;display:flex;align-items:center;gap:12px;background:var(--background);padding:8px 12px;border-radius:8px;box-shadow:0 2px 8px rgba(0,0,0,.1)"><a href=https://www.linkedin.com/in/james-rea/ target=_blank rel="noopener noreferrer" title=LinkedIn style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://github.com/jfgrea27 target=_blank rel="noopener noreferrer" title=GitHub style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=mailto:jfgrea27@gmail.com title=Email style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a href=/James_Rea___CV___December_2024___v4.pdf target=_blank title=CV style=color:inherit><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
</a><a href=https://www.buymeacoffee.com/jfgrea27 target=_blank><img src=https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png alt="Buy Me A Coffee" style=height:40px!important;width:145px!important></a></div><div class=content><header><div class=header></div></header><main class=main><div class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/>Posts</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/06-ml-math/>Maths behind ML and AI</a>
<span class=breadcrumbs-separator>> </span><a class=breadcrumbs-current href=/posts/06-ml-math/01-linear-regression/>01 - Linear regression</a></div><div><div class=single-intro-container><h1 class=single-title>01 - Linear regression</h1><p class=single-summary>Fitting a line of best fit.</p><p class=single-readtime><time datetime=2026-01-31T00:00:00+00:00>January 31, 2026</time></p></div><div class=single-content><h2 class=heading id=problem>Problem
<a class=anchor href=#problem>#</a></h2><p>Suppose you are trying to determine the relationship between a person&rsquo;s height and their shoe size. You would like to answer the question: Given a person&rsquo;s shoe size $X$, what do I expect their height $Y$ to be?</p><p>Linear regression tries to answer this by creating a <strong>line of best fit</strong> between your independent variable(s) $X$ and your dependent variables ($Y$).</p><p>This linear line of best fit will have the following shape:</p><p>$$
\begin{equation}
\hat{Y} = \theta_{1}X + \theta_{0}
\end{equation}
$$</p><p>where $\hat{Y}$ represents the hypothesis height for our given $X$.
Here $\theta_{1}$ is the gradient of the line and $\theta_{0}$ the intercept on the y-axis.</p><p>This is illustrated in the following animation:</p><img src=scatter-plot.gif alt="Animation of what linear regression aims to find" style="max-width:600px;display:block;margin:0 auto"><p>So to find the line of best fit, we need to determine the optimum values for $\theta_{0}, \theta_{1}$.</p><p>Geometricaly, as noted by the yellow lines, the line of best fit actually as the smallest distance between the line and $Y$ coordinate.</p><img src=y-distance.gif alt="Distance between observed y and hypothesis" style="max-width:600px;display:block;margin:0 auto"><p>Hence, if we try to reduce that distance, we will get the line of best fit.</p><p>For each $y_{i} \in Y$, the distance between $y_{i} - \hat{y{i}}$ can either be negative or positive. We can square the distance so that we always have a positive distance. This will prevent negative distances cancelling out the work of reducing the parameters $\theta_{0}, \theta_{1}$.</p><h2 class=heading id=mean-square-error>Mean Square Error
<a class=anchor href=#mean-square-error>#</a></h2><p>Thus, what we are trying to do is find the smallest average of all the distances $y_{i} - \hat{y_{i}}$. This is know as the <strong>Mean Square Error</strong>.</p><p>More formally, we can write:</p><p>$$</p><p>MSE = \frac{1}{m}\sum_{i=1}^{m}(Y - \hat{Y})^2 = \frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2
$$</p><p>where the last equality holds by (1) above.</p><p>Hence, we can create the following cost function:</p><p>$$
\begin{equation}
J(\theta_{0}, \theta_{1}) = \frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2
\end{equation}
$$</p><p>$$
\begin{equation}
Cost = \min(J(\theta_{0}, \theta_{1}))
\end{equation}
$$</p><p>As you can see, $J$ only has 2 variables (namely $\theta_{0}, \theta_{1}$), we can treat $X, Y$ as constants.</p><p>If we were to plot a 3-D graph of $f: (\theta_{0}, \theta_{1}) \rightarrow J(\theta_{0}, \theta_{1})$, it will look something like</p><img src=cost-surface.gif alt="A plot of cost function" style="max-width:600px;display:block;margin:0 auto"><p>This 3-D parabolic surface has a clear minima. This minima represents the the lowest cost (z-axis).</p><p>Suppose we start with arbitrary point on the surface, then we would try to iterate moving towards the minima of the surface as illustrated. This is known as <strong>Gradient Descent</strong> and is a very common technique in optimization to determine best solutions to cost functions.</p><p>Note: This works here because the surface has a nice shape, namely it is <strong>convex</strong>, which means that the second derivative is not negative. This means we can always descend closer to the minima of the cost. Other projections of cost functions are <strong>concave</strong>, which makes it impossible to apply gradient descent.</p><h2 class=heading id=derivation>Derivation
<a class=anchor href=#derivation>#</a></h2><p>For each variable $\theta_{0}, \theta_{1}$, we need to</p><ol><li>find the gradient at that point with respect to the given variable.</li><li>move some amount in a negative (towards the minima) of the surface.</li></ol><p>More formally, this can be written as</p><p>$$
\begin{equation}
\theta_{0}^k = \theta_{0}^{k-1} - \alpha\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{0}}}
\end{equation}
$$</p><p>$$
\begin{equation}
\theta_{1}^k = \theta_{1}^{k-1} - \alpha\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{1}}}
\end{equation}
$$</p><p>where $\alpha$ is a hyperparameter that represents the rate at which we move towards the minima.</p><p>Let&rsquo;s derive the partial derivatives!</p><p>Looking at $\theta_{0}$ initially:</p><p>$$
\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{0}}} = \frac{\partial{\frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2}}{\partial{\theta_{0}}}
$$</p><p>as defined in (2).</p><p>Let $u =\frac{1}{m}\sum_{i=1}^{m}(Y - \theta_{1}X + \theta_{0})$, and so $\frac{\partial{u}}{\partial{\theta_{0}}} = -1$, then by the <strong>Chain Rule</strong>:</p><p>$$
\frac{\partial{\frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2}}{\partial{\theta_{0}}} = \frac{\partial{J(u)}}{\partial{u}} . \frac{\partial{u}}{\theta_{0}} = \frac{1}{m}2mu . -1 = -2u
$$</p><p>Here $\frac{\partial{J(u)}}{\partial{u}} = 2m$ since we are summing across all points.</p><p>Finally, we substitute $u$ to get</p><p>$$
\begin{equation}
\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{0}}} = \frac{-2}{m}\sum_{i=1}^{m}(Y - \hat{Y})
\end{equation}
$$</p><p>Similarly for $\theta{1}$:</p><p>$$
\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{1}}} = \frac{\partial{\frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2}}{\partial{\theta_{1}}}
$$</p><p>as defined in (2).</p><p>Let $u =\frac{1}{m}\sum_{i=1}^{m}(Y - \theta_{1}X + \theta_{0})$, and so $\frac{\partial{u}}{\partial{\theta_{1}}} = -X$, then by the <strong>Chain Rule</strong>:</p><p>$$
\frac{\partial{\frac{1}{m}\sum_{i=1}^{m}(Y - (\theta_{1}X + \theta_{0}))^2}}{\partial{\theta_{1}}} = \frac{\partial{J(u)}}{\partial{u}} . \frac{\partial{u}}{\theta_{1}} = \frac{1}{m}2mu . -X = -2uX
$$</p><p>Here again, $\frac{\partial{J(u)}}{\partial{u}} = 2m$ since we are summing across all points.</p><p>Finally, we substitute $u$ to get</p><p>$$
\begin{equation}
\frac{\partial{J(\theta_{0}, \theta_{1})}}{\partial{\theta_{1}}} = \frac{-2}{m}\sum_{i=1}^{m}(Y - \hat{Y})*X
\end{equation}
$$</p><p>Therefore, overall we have the following:</p><p>$$
\begin{equation}
\theta_{0}^k = \theta_{0}^{k-1} - \alpha\frac{-2}{m}\sum_{i=1}^{m}(Y - \theta_{1}X + \theta_{0})
\end{equation}
$$</p><p>$$
\begin{equation}
\theta_{1}^k = \theta_{1}^{k-1} - \alpha\frac{-2}{m}\sum_{i=1}^{m}(Y - \theta_{1}X + \theta_{0})* X
\end{equation}
$$</p><p>Since $\alpha$ is arbitrary, we can omit the $\frac{2}{m}$.</p><h2 class=heading id=code>Code
<a class=anchor href=#code>#</a></h2><p>Let&rsquo;s look at a real example in <code>Python</code>. We will look at at <a href=https://www.kaggle.com/datasets/abhishek14398/salary-dataset-simple-linear-regression>Salary Dataset</a> from kaggle.com.</p><p>For this, you will require</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-txt data-lang=txt><span style=display:flex><span>matplotlib
</span></span><span style=display:flex><span>numpy
</span></span><span style=display:flex><span>pandas
</span></span></code></pre></div><p>The data looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-py data-lang=py><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>pandas</span> <span style=font-weight:700;text-decoration:underline>as</span> <span style=color:#666;font-weight:700;font-style:italic>pd</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>matplotlib.pyplot</span> <span style=font-weight:700;text-decoration:underline>as</span> <span style=color:#666;font-weight:700;font-style:italic>plt</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic># Read the salary dataset</span>
</span></span><span style=display:flex><span>df = pd.read_csv(<span style=color:#666;font-style:italic>&#39;path/to/salary.csv&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X = df[<span style=color:#666;font-style:italic>&#39;YearsExperience&#39;</span>].to_numpy()
</span></span><span style=display:flex><span>y = df[<span style=color:#666;font-style:italic>&#39;Salary&#39;</span>].to_numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic># Create scatter plot</span>
</span></span><span style=display:flex><span>plt.figure(figsize=(10, 6))
</span></span><span style=display:flex><span>plt.scatter(df[<span style=color:#666;font-style:italic>&#39;YearsExperience&#39;</span>], df[<span style=color:#666;font-style:italic>&#39;Salary&#39;</span>], color=<span style=color:#666;font-style:italic>&#39;blue&#39;</span>, alpha=0.7)
</span></span><span style=display:flex><span>plt.xlabel(<span style=color:#666;font-style:italic>&#39;Years of Experience&#39;</span>)
</span></span><span style=display:flex><span>plt.ylabel(<span style=color:#666;font-style:italic>&#39;Salary ($)&#39;</span>)
</span></span><span style=display:flex><span>plt.title(<span style=color:#666;font-style:italic>&#39;Salary vs Years of Experience&#39;</span>)
</span></span><span style=display:flex><span>plt.grid(<span style=font-weight:700;text-decoration:underline>True</span>, alpha=0.3)
</span></span><span style=display:flex><span>plt.tight_layout()
</span></span><span style=display:flex><span>plt.savefig(<span style=color:#666;font-style:italic>&#39;salary_plot.png&#39;</span>)
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p>This should look something like:</p><img src=salary-plot.png alt="A years of experience vs salary plot" style="max-width:600px;display:block;margin:0 auto"><p>Let&rsquo;s initialize $\theta_{0}, \theta_{1}$ and apply gradient descent</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#888;font-style:italic># previous part above</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic># Initialize parameters</span>
</span></span><span style=display:flex><span>theta_0 = 0.0  <span style=color:#888;font-style:italic># y-intercept</span>
</span></span><span style=display:flex><span>theta_1 = 0.0  <span style=color:#888;font-style:italic># gradient</span>
</span></span><span style=display:flex><span>alpha = 0.0001 <span style=color:#888;font-style:italic># learning rate</span>
</span></span><span style=display:flex><span>n_iterations = 1000
</span></span><span style=display:flex><span>m = <span style=font-weight:700;font-style:italic>len</span>(X) <span style=color:#888;font-style:italic># number of samples</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic># Store loss history</span>
</span></span><span style=display:flex><span>losses = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic># Gradient descent</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> i <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>range</span>(n_iterations):
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic># Predictions</span>
</span></span><span style=display:flex><span>    y_pred = theta_0 + theta_1 * X
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic># Calculate MSE loss</span>
</span></span><span style=display:flex><span>    loss = (1 / m) * np.sum((y - y_pred) ** 2)
</span></span><span style=display:flex><span>    losses.append(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic># Calculate gradients</span>
</span></span><span style=display:flex><span>    d_theta_0 = (-2 / m) * np.sum(y - y_pred) <span style=color:#888;font-style:italic># equation 6</span>
</span></span><span style=display:flex><span>    d_theta_1 = (-2 / m) * np.sum((y - y_pred) * X) <span style=color:#888;font-style:italic># equation 7</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic># Update parameters</span>
</span></span><span style=display:flex><span>    theta_0 = theta_0 - alpha * d_theta_0 <span style=color:#888;font-style:italic># equation 8</span>
</span></span><span style=display:flex><span>    theta_1 = theta_1 - alpha * d_theta_1 <span style=color:#888;font-style:italic># equation 9</span>
</span></span></code></pre></div><p>The plot below shows the loss and the line of best fit:</p><img src=linear_regression_results.png alt="Plot of line of best fit and loss" style="max-width:600px;display:block;margin:0 auto"><p>So there you have it, linear regression explained from first principles!</p><h2 class=heading id=more-discussion>More discussion
<a class=anchor href=#more-discussion>#</a></h2><p>It is worth mentioning that choosing a right <strong>learning rate</strong> $\alpha$ is crucial to help performance.</p><p>A too small will take too long, a too big may oscillate or diverge (miss the minima completely). The following is a rule of thumb:</p><table><thead><tr><th>Learning rate</th><th>Behavior</th></tr></thead><tbody><tr><td>Very small (e.g. 0.0001)</td><td>Slow convergence</td></tr><tr><td>Moderate (e.g. 0.01)</td><td>Stable, efficient learning</td></tr><tr><td>Large (e.g. 1.0)</td><td>Divergence / oscillation</td></tr></tbody></table></div><div class=back-to-top><a href=#top>back to top</a></div></div></main></div><footer><p>Powered by
<a href=https://gohugo.io/>Hugo</a>
and
<a href=https://github.com/tomfran/typo>tomfran/typo</a></p></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body><script src=/js/theme-switch.js></script><script defer src=/js/copy-code.js></script></html>